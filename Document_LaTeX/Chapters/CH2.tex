% !TEX root = ../main.tex

\chapter{Solution Theory} % Main chapter title
\label{theory} % \ref{theory}


This chapter addresses the solution concepts for the problems that needed to be solved in order to realize the project and is structured according to the previously mentioned main components of the project: object detection, coordinate transformation, and robotic arm control. 
 
\section{Object detection}
The first component of the project is the object detection. Its purpose is to detect objects in the workspace and determine their relative coordinates and size in the image as well as their orientation in relation to the table. 

To simplify these problems, we decided to use a top-down view of the workspace. This means that the camera is positioned above the workspace, so that a linear correlation between the image and the table coordinates emerges. At the early stages of project development the training of a custom object detection model was not intended and it was planned to utilize an existing model. The YOLOv3 model, a convolutional neural network that is trained to detect objects in images, was selected due to its wide array of object classes and its high performance.

To detect the orientation of an object relative to the table, we decided to use OpenCV, a python library used in computer vision applications which provides a broad array of functions for image processing. The main idea was to determine the contours of the object by converting the image into the HSV color space and applying different filter. The contours are then used to calculate the main orientation of the object, using principle component analysis. 

\section{Coordinates transformation}

Once knowing the coordinates of the detected object in the image, 
the next step is to transform the objects' position vector from the image coordinate system to the world's coordinate system. This type of transformation is best achieved with the use of a transformation matrix.


\subsection{Transformation Matrix}

Assuming a current coordinate system $\mathbf{A}$ and a target coordinate system $\mathbf{B}$, the transformation matrix $\mathbf{T}$ can be used to transform a vector $\vec{v}$ from $\mathbf{A}$ to $\mathbf{B}$.
A transformation matrix can be represented as a matrix frame, built from a combination of a rotation matrix, a translation vector, a scaling vector and a perspective projection matrix.

$$
\mathbf{T} = \left[
\begin{array}{ccc|c}
\ast&\ast       &\ast&\ast\\
\ast&\mathbf{R} &\ast&\vec{t}  \\
\ast&\ast       &\ast&\ast\\
\hline
\ast&\mathbf{P} &\ast&\mathbf{S}
\end{array}
\right]
$$


\begin{conditions}
    \mathbf{R} & the rotation matrix with the dimensions $3\times 3$.  \\
    \vec{t} & the translation vector with the dimensions $3\times 1$.  \\
    \mathbf{P} & the perspective projection matrix with the dimensions $1\times 3$.  \\
    \mathbf{S} & the scale factor with the dimensions $1\times 1$ (for uniform or isotropic scaling).\\
\end{conditions}

The rotation matrix for a rotation around any given axis given by the unit vector $\mathbf{\vec{u}(x,y,z)}$ by an angle $\theta$ is given by the following formula:

$$
\mathbf{R} =
\begin{bmatrix}
u_x^2(1-\cos\theta) + \cos\theta & u_xu_y(1-\cos\theta) - u_z\sin\theta & u_xu_z(1-\cos\theta) + u_y\sin\theta \\
u_xu_y(1-\cos\theta) + u_z\sin\theta & u_y^2(1-\cos\theta) + \cos\theta & u_yu_z(1-\cos\theta) - u_x\sin\theta \\
u_xu_z(1-\cos\theta) - u_y\sin\theta & u_yu_z(1-\cos\theta) + u_x\sin\theta & u_z^2(1-\cos\theta) + \cos\theta \\
\end{bmatrix}
$$

If the rotation is performed around the z axis, the rotation matrix can be simplified to the following form:
$$
\mathbf{R} =
\begin{bmatrix}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
$$


The translation vector is the position of the origin from current coordinate system $O_{current}$ 
relative to the target coordinate system $O_{target}$, i.e. the distance between both origins given as a three dimensional vector. 

$$
\vec{t} =
\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}=
O_{current}-O_{target}
$$

The perspective projection matrix is not used in this project due to the camera orientation being perpendicular to the surface of intereset, but is included for completeness.

The scaling factor as given in the frame above can only be used for isotropic scaling, i.e. scaling in all three dimensions by the same factor. Since we are mainly interested in scaling in the x and y directions by different amounts, a single scaling factor can not be used by itself and needs to be expanded to a scaling matrix $\mathbf{S}$ with the following form:

$$
\mathbf{S}=
\begin{bmatrix}
S_x & 0 & 0   & 0\\
0 & S_y & 0   & 0\\
0 & 0   & S_z & 0\\
0 & 0   & 0   & 1
\end{bmatrix}
$$


\begin{conditions}
    S_x & scaling factor in x dimension \\
    S_y & scaling factor in y dimension \\
    S_z & scaling factor in z dimension   \\
\end{conditions}


With the rotation and translation matrices a primary transformation matrix $\mathbf{T_{0}}$ is built with the previously described frame using the unit 1 as a scaling factor and a null matrix as the perspective matrix. 
This is then multiplied with the scaling matrix $\mathbf{S}$ to obtain the complete transformation matrix $\mathbf{T}$ as follows. 

$$\mathbf{T} =  \mathbf{S} \bullet  \mathbf{T_{0}}$$

The position of a point from the current coordinate system $\mathbf{A}$ can be transformed to the target coordinate system $\mathbf{B}$ using the Transformation matrix $\mathbf{T}$ as follows:

$$\vec{v_{B}} = \mathbf{T} \bullet \vec{v_{A}}$$

\section{Robot controller}

The robot controller is the main program where the behaviour of the robot is defined. It is responsible for the different actions that take place in the simulation. By marking the robot as a supervisor in webots, it can access and modify the properties of other elements in the scene and the environment. 

The main tasks of the robot controller are the following:

\begin{itemize}
    \item Initialization of the different modules and devices
    \item calling of the image processing and object detection modules
    \item performing the required movements of the robotic arm
    \item controlling the movement of the gripper and its fingers
    \item coordinating the actions of the different modules
\end{itemize}

\subsection{Robot Kinematics}

The Robot chosen for this task consists of a robotic arm with a gripper attached to the end of the arm. The robot by itself can be represented as a kinematic chain with 6 joints, resulting in 6 degrees of freedom.
In order to move the robot, the target angle of each individual joint in the kinematic chain needs to be set. Nevertheless, the position of the objects and target locations for the robot's movement are defined in a three dimensional coordinate system, so a transformation between the joint angles and the position of the end effector (last chain element) in space needs to be achieved in order to control its movements.

Calculating the position of the end effector (in this case the gripper) from the position of the individual motors can be achieved using a process called forward kinematics, which combines multiple applications of trigonometric formulas.

Nonetheless, the reverse operation, which aims to calculate the required position of the joints in the kinematic chain needed to reach a given position with the end effector presents a more challenging problem. 
This process called inverse kinematics (IK), for which different methods can be used. These methods can be divided into the categories analytical and numerical (\Vref{fig:fkik}). \autocite{robotik}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/fk_vs_ik.png}
    \caption{Forward and inverse kinematics shown with the green and red arrows respectively.}
    \label{fig:fkik}
\end{figure}

The analytical methods are based on the use of trigonometric formulas, which can be used to calculate the required position values for the motors. However, these methods are limited to a specific number of degrees of freedom, and can only be used for a limited number of cases.

The numerical methods are based on the use of iterative algorithms, which can be used to calculate the required position values for the motors. However, while these methods are not limited to a specific number of degrees of freedom, they can be computationally expensive and are non-deterministic procedures, meaning there can be more than one solution for a given point. Likewise, the time required to find a solution is also non-deterministic, which poses a problem when critical computations need to be performed in real time.


\subsection{Gripper Actuation}

The gripper consists of three individual fingers, each of them having three joints. The closing action of the finger is achieved by rotating the first joint of each finger until the desired object is grabed or the maximum joint rotation is reached. This alone creates a claw like grip, since all finger sections rotate with the first joint relative to the global coordinates. While useful in some cases, its contact area with the object being grabbed is significantly reduced.

A different approach that aims to improve the contact area of the gripper, performs a rotation on the last joint of each finger in the opposite direction by the same amount as the first joint, compensating the rotation on the fingertips and creating a more stable grip with a flat surface area.

\Vref{fig:grip_states} shows the gripper in the open and closed position using the a claw and a pinch grip.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{Figures/red_color.jpg}
%     \caption{From left to rigth: Gripper in open, closed claw, and closed flat positions.}
%     \label{fig:gripper_states}
% \end{figure}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \adjincludegraphics[width=\textwidth]{Figures/grip_open.png}   
        \caption{Open Grip}
    \end{subfigure}\\\vspace{0.5cm}
    
    \begin{subfigure}[b]{0.4\textwidth}
        \adjincludegraphics[width=\textwidth]{Figures/grip_claw.png}        
        \caption{Claw Grip}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \adjincludegraphics[width=\textwidth]{Figures/grip_flat.png}        
        \caption{Pinch Grip}
    \end{subfigure}\\\vspace{0.5cm}

    \caption{Comparison between the open (resting) position of the gripper with the closed claw and pinch grip methods.}
    \label{fig:grip_states}
\end{figure}

\subsection{Movement coordination}

The main loop of actions that need to be taken to complete the task of organizing the desk can be defined as seen in \vref{fig:flowchart}. 

The process starts by moving the robot to a HOME position where it wont interfere with the camera's view of the desk. A picture is taken and any objects present are detected. If objects were detected, the robot iterates through the found objects, picking them up and placing them on the a second table on their corresponding position, until all the detected objects have been moved. The robot then moves back to the HOME position. If no object is detected, no movement of the robot takes place. This process is repeated until the user decides to stop the simulation.

% \begin{lstlisting}[]%language=plantuml
% @startuml
% :**Simulation Start**;
% repeat :Move Robot to HOME Position;
%     repeat :Take Picture;
%         :Perform Object Detection;
%     repeat while (objects found?) is (no) not (yes)

%     repeat :Coordinate Transformation;
%         :Move object;
%         :Remove from found objects;
%     repeat while (more objects?) is (yes)
% repeat while (Keep Watching?) is (yes)
% :stop;
% @enduml
% \end{lstlisting}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/sim_loop.png}
    \caption{Flowchart of the main loop of actions}
    \label{fig:flowchart}
\end{figure}

